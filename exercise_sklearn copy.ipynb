{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Scikit-Learn\n",
    "\n",
    "## Gradescope Autograding\n",
    "\n",
    "Please follow [all standard guidance](../autograder_guidelines.ipynb) for submitting this assignment to the Gradescope autograder, including storing your solutions in a dictionary called `results` and ensuring your notebook runs from the start to completion without any errors.\n",
    "\n",
    "For this assignment, please name your file `exercise_sklearn.`ipynb` before uploading.\n",
    "\n",
    "You can check that you have answers for all questions in your `results` dictionary with this code:\n",
    "\n",
    "```python\n",
    "assert set(results.keys()) == {\n",
    "    \"ex3_num_columns\",\n",
    "    \"ex6_model_smoking\",\n",
    "    \"ex7_first_prediction\",\n",
    "    \"ex8_score\",\n",
    "    \"ex9_SVR_score\",\n",
    "    \"ex10_SVR_linear_score\",\n",
    "}\n",
    "```\n",
    "\n",
    "### Submission Limits\n",
    "\n",
    "Please remember that you are **only allowed three submissions to the autograder.** Your last submission (if you submit 3 or fewer times), or your third submission (if you submit more than 3 times) will determine your grade Submissions that error out will **not** count against this total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy as pt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pprint as pp\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning in Medicine\n",
    "\n",
    "In these exercises, we'll learn to fit and evaluate (in a basic way) machine learning models using the package `scikit-learn`.\n",
    "\n",
    "The emphasis of these exercises is to help you get comfortable with the data-wrangling component of machine learning so that in future courses you can focus on the *theory* underlying machine learning. With that in mind, we will be quite cavalier with model fitting and evaluation. As with our `statsmodels` exercises—in which we quickly ran through a few models to practice model *implemention* without thinking too much about model selection and specification—this is **not** how you should do your actual data science analyses!\n",
    "\n",
    "Though this is true generally, it is doubly true in the context of these exercises: the application of machine learning to medicine.\n",
    "\n",
    "In these exercises, we will use the birth-weight data we used for our `statsmodels` exercises to predict birth weights. As you will see, implementing a machine learning model to predict birth weights is actually surprisingly straightforward. But that ease is deceptive; while machine learning algorithms are easy to use, they're hard to use *well*, and if you get them wrong in contexts where they impact real people, poorly implemented machine learning models can literally kill people.\n",
    "\n",
    "Lest you think I'm being hyperbolic, consider the case of a machine learning model used by medical providers across the US to make treatment decisions for millions of people. The goal of the model (distributed by a company called Optum) was to help providers figure out what patients were especially likely to face health problems down the road, so they could provide these patients extra preventative care.\n",
    "\n",
    "The problem, though, is that the way Optum did this was by training a supervised machine learning model to predict future healthcare use by patients. Patients the model predicted would consume more healthcare in the future, the model implicitly *assumed*, were those who should get extra care today.\n",
    "\n",
    "But [as was recently described in a paper in the journal Science](https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/), the problem is that the model had a large racial bias, and was less likely to recommend extra preventative care for black patients.\n",
    "\n",
    "The reason was that Black patients in the United States tend to use the medical system less than White patients *even when the Black patients have the same level of medical need as White patients*. Why? The paper doesn't explore that fully, but the causes likely include skepticism towards the medical establishment due to the history of Black Americans being used as [unknowing test subjects for medical studies](https://en.wikipedia.org/wiki/Tuskegee_syphilis_experiment), or the fact that Black Americans tend to have lower incomes and are less likely to be insured than White Americans.\n",
    "\n",
    "So when this model saw that Black patients didn't consume as much healthcare in the future, the algorithm interpreted that as evidence that Black patients *didn't need* healthcare in the future. This resulted in the model falsely telling doctors that Black patients were systematically less in need of preventative care.\n",
    "\n",
    "**Crucially, this occurred even though race wasn't even a variable in the model.** Machine learning models are very good at picking up subtle signals, and so even though patient race wasn't an explicit factor in the model, the model was nevertheless able to differentiate Black and White patients. Though it's not clear exactly how it did so, this can happen whenever variables are included in models that are *correlated* with race. For example, people's zip codes (which identify where people live) are notorious for proxying for race in machine learning algorithms since residential segregation in the US means that most people in a given zip code are of the same race.\n",
    "\n",
    "And so as a result, this well-meaning machine learning algorithm resulted in black patients receiving fewer preventative medical interventions than white patients, even after taking into account other (medically relevant) factors.\n",
    "\n",
    "So: in this exercise, we'll play with predicting birth weights in infants. But do **NOT** think that just because it's this easy to fit a model, it is appropriate to then go use these in the real world in contexts where people's lives are affected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**(1)** Load the data \"smoking.csv\", which includes information on both biometrics of infants at birth, and information on mothers (variables prefixed with the letter \"m\"), from [this MIDS repo](https://github.com/nickeubank/MIDS_Data). We'll be working with this data in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>gestation</th>\n",
       "      <th>bwt.oz</th>\n",
       "      <th>parity</th>\n",
       "      <th>mrace</th>\n",
       "      <th>mage</th>\n",
       "      <th>med</th>\n",
       "      <th>mht</th>\n",
       "      <th>mpregwt</th>\n",
       "      <th>inc</th>\n",
       "      <th>smoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4604</td>\n",
       "      <td>1598</td>\n",
       "      <td>148</td>\n",
       "      <td>116</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7435</td>\n",
       "      <td>1527</td>\n",
       "      <td>181</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7722</td>\n",
       "      <td>1563</td>\n",
       "      <td>204</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>140</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026</td>\n",
       "      <td>1503</td>\n",
       "      <td>225</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553</td>\n",
       "      <td>1638</td>\n",
       "      <td>233</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  date  gestation  bwt.oz  parity  mrace  mage  med  mht  mpregwt  inc  \\\n",
       "0  4604  1598        148     116       7      7    28    1   66      135    2   \n",
       "1  7435  1527        181     110       7      7    27    1   64      133    1   \n",
       "2  7722  1563        204      55      11      7    35    3   65      140    6   \n",
       "3  2026  1503        225     132       4      7    28    2   67      148    3   \n",
       "4  3553  1638        233     105       4      7    34    3   61      130    3   \n",
       "\n",
       "   smoke  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "df_raw = pd.read_csv(\n",
    "    \"https://media.githubusercontent.com/media/nickeubank/MIDS_Data/refs/heads/master/smoking.csv\"\n",
    ")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Your Data\n",
    "\n",
    "Unlike in `statsmodels`, we can't use `pandas` DataFrames in scikit-learn, so the first step in nearly any machine learning workflow (if you haven't already been given a nice giant numpy array) is to convert our heterogeneous pandas array (which includes strings, categorical variables, integers, and floating point numbers) into a single large matrix that consists only of floating point numbers. \n",
    "\n",
    "While you can do this by hand, this is most easily accomplished using the `patsy` library, which will take a pandas array and a special formula string and return numpy arrays for use in libraries like scikit-learn. (`patsy` is actually the library that implemented the formulas we used in `statsmodels` to specify our regression models—here we're just using it on its own).\n",
    "\n",
    "Let's assume that for most of these exercises, we want to predict birth weight (`bwt.oz`) using:\n",
    "\n",
    "- whether the mother is (a) White, (b) Black, (c) Hispanic or (d) of another ethnicity (in other words, please recode `mrace` into four categories)\n",
    "- whether the mother smokes (`smoke`)\n",
    "- Mother's age (`mage`)\n",
    "- Mother's weight (`mpregwt`)\n",
    "- Mother's height (`mht`)\n",
    "\n",
    "We won't use any interaction terms in this exercise, just these entered on their own.\n",
    "\n",
    "For race, recall that in the raw data, `mrace` is coded as:\n",
    "\n",
    "```\n",
    "mrace    mother’s race or ethnicity\n",
    "         0-5= white\n",
    "         6  = mexican\n",
    "         7 = black\n",
    "         8 = asian\n",
    "         9 = mix\n",
    "         99 = unknown\n",
    "```\n",
    "\n",
    "Re-group mrace to be White(0-5), Hispanic (6), Black(7), other (8, 9, 99)\n",
    "\n",
    "(We're ignoring `gestation` because we don't really know the value of `gestation` before the child is born, so we can't use it to predict the birthweight of not-yet-born children!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1, 2, 3, 4], dtype='int64')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df[\"mrace\"] = np.where(\n",
    "    df_raw[\"mrace\"].isin([0, 1, 2, 3, 4, 5]),\n",
    "    1,\n",
    "    np.where(\n",
    "        df_raw[\"mrace\"].isin([8, 9, 99]),\n",
    "        4,\n",
    "        np.where(df_raw[\"mrace\"] == 6, 2, np.where(df_raw[\"mrace\"] == 7, 3, 99)),\n",
    "    ),\n",
    ")\n",
    "\n",
    "df[\"mrace\"] = df[\"mrace\"].astype(\"category\")\n",
    "df[\"mrace\"].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Begin by using `patsy.dmatrices()` to create two datasets (`y`, which is the birth weights, and `X`, which is a matrix with all your \"features\" in a nice numpy array).\n",
    "\n",
    "**Note:** You may hit a similar issue using patsy here you encountered when writing formulas in the [statsmodels](exercises/statsmodel.ipynb) exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>gestation</th>\n",
       "      <th>bwtoz</th>\n",
       "      <th>parity</th>\n",
       "      <th>mrace</th>\n",
       "      <th>mage</th>\n",
       "      <th>med</th>\n",
       "      <th>mht</th>\n",
       "      <th>mpregwt</th>\n",
       "      <th>inc</th>\n",
       "      <th>smoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4604</td>\n",
       "      <td>1598</td>\n",
       "      <td>148</td>\n",
       "      <td>116</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7435</td>\n",
       "      <td>1527</td>\n",
       "      <td>181</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7722</td>\n",
       "      <td>1563</td>\n",
       "      <td>204</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>140</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026</td>\n",
       "      <td>1503</td>\n",
       "      <td>225</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553</td>\n",
       "      <td>1638</td>\n",
       "      <td>233</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>7781</td>\n",
       "      <td>1616</td>\n",
       "      <td>324</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>4344</td>\n",
       "      <td>1559</td>\n",
       "      <td>328</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>125</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>3917</td>\n",
       "      <td>1703</td>\n",
       "      <td>329</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>7707</td>\n",
       "      <td>1592</td>\n",
       "      <td>330</td>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>7152</td>\n",
       "      <td>1495</td>\n",
       "      <td>330</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>130</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  date  gestation  bwtoz  parity mrace  mage  med  mht  mpregwt  inc  \\\n",
       "0    4604  1598        148    116       7     3    28    1   66      135    2   \n",
       "1    7435  1527        181    110       7     3    27    1   64      133    1   \n",
       "2    7722  1563        204     55      11     3    35    3   65      140    6   \n",
       "3    2026  1503        225    132       4     3    28    2   67      148    3   \n",
       "4    3553  1638        233    105       4     3    34    3   61      130    3   \n",
       "..    ...   ...        ...    ...     ...   ...   ...  ...  ...      ...  ...   \n",
       "864  7781  1616        324    117       1     1    22    1   62      164    2   \n",
       "865  4344  1559        328    117       1     1    29    2   65      125    4   \n",
       "866  3917  1703        329    144       3     4    22    2   65      190    2   \n",
       "867  7707  1592        330    105       3     3    23    2   64      112    4   \n",
       "868  7152  1495        330    132       1     1    34    4   64      130    5   \n",
       "\n",
       "     smoke  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "864      1  \n",
       "865      1  \n",
       "866      1  \n",
       "867      1  \n",
       "868      1  \n",
       "\n",
       "[869 rows x 12 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"bwt.oz\": \"bwtoz\"})\n",
    "target_vector, feature_matrix = pt.dmatrices(\"bwtoz~mrace+mage+mht+mpregwt+smoke\", df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Look at your features matrix `X`. How many columns does it have? Store your answer as `ex3_num_columns`. How does that compare to the number of variables you used as inputs? (if they're the same, you probably did something wrong...). \n",
    "\n",
    "Explain all the things patsy has done with your data (hint: it's made TWO changes affecting the number of columns in your data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns for the feature matrix X is 8.\n",
      "['Intercept', 'mrace[T.2]', 'mrace[T.3]', 'mrace[T.4]', 'mage', 'mht', 'mpregwt', 'smoke']\n"
     ]
    }
   ],
   "source": [
    "no_of_columns = feature_matrix.shape[1]\n",
    "results[\"ex3_num_columns\"] = no_of_columns\n",
    "print(f\"The number of columns for the feature matrix X is {no_of_columns}.\")\n",
    "feature_matrix\n",
    "print(feature_matrix.design_info.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I used five (5) variables and patsy gave a design matrix with eight (8) columns. Patsy did the following operations (based on its default behavior):\n",
    "> 1. It added the intercept term as an additional column of ones.\n",
    "> 1. Because mother's race (`mrace`) is a categorical column, it converted its four (4) categories into three (3) dummy variable in 3 columns to represent the presence of each category. The 4th category is made the reference category. That is normally the first category when the categories are storted.\n",
    "> 1. It added the 4 numerical variables, mother's age (`mage`), mother's height (`mht`), mothers weight during pregnancy (`mpregwt`), if mother smokes (`smoke`), as four (4) columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Your Data\n",
    "\n",
    "In machine learning, model selection is often accomplished by: \n",
    "\n",
    "1. Splitting your data into two parts (a training set and a test set), \n",
    "2. Training your model on the training set (i.e. set the parameters of your model to best explain the training data).\n",
    "3. Testing the model by using the parameters generated during that training to predict values for the testing data, then comparing the predicted values for the testing data to the actual values in the test data. The smaller the difference between the predicted values and the true values in the testing data, the better the model is said to perform.\n",
    "\n",
    "Suppose we just wanted to use linear regression as our model. First, we'd randomly pick 80% our data, then regress birth weight on the various variables (\"features\" in machine learning terminology) we specified above. Then we'd use the coefficients from that regression to predict birth weights for the 20% of children we didn't use in our estimation, and see how different those predictions are from actual birth weights. If we find a model that performs well on our testing data, then we *assume* (*hope*) that that model will also work well on new data (i.e. on children who haven't been born yet whose weight we want to predict).\n",
    "\n",
    "(Readers from a statistics background will recognize this as being a little analogous to \"cross-validation.\")\n",
    "\n",
    "So the first step in machine learning is to split our sample! Thankfully this is easy to do with the `sklearn` `train_test_split` function. Import the function with `from sklearn.model_selection import train_test_split`, and split your data. To give you a sense of how it works, this is a common syntax:\n",
    "\n",
    "```python \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "```\n",
    "\n",
    "Where `X_train` is your training features, `Y_train` are your training birth weights, `X_test` are your test features, and `Y_test` are your test birth weights. The `random_state` var just ensures that you can re-create this split if you have to re-run your code (helpful for debugging and for our autograder).\n",
    "\n",
    "**(4)** Start by splitting YOUR data, putting 80% into the training data. Set your random state with the seed `42` so your random split is the same as will be used by the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   0.,   0.,   0.,  37.,  66., 135.,   1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    feature_matrix, target_vector, test_size=0.2, train_size=0.8, random_state=42\n",
    ")\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model! \n",
    "\n",
    "`scikit-learn` is much loved because it has, like... every model ever already built in, and it provides a common interface (API) for all of them. Seriously—go check out all the supervised machine learning models that come [with scikit-learn here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). \n",
    "\n",
    "Moreover, unlike many open-source projects, all of its models are really well documented, so you can read all about them. And [check out this nifty guide to choosing an appropriate model](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "For this exercise, let's start by fitting a `LinearRegression` model. \n",
    "\n",
    "Wait, you say: isn't that what we did in `statsmodels`? Yes! \n",
    "\n",
    "Data Science is a very fragmented discipline, and some stuff gets recapitulated in slightly different ways in different places, so it's common to see different implementations of the same thing as you move from the world of statisticians (`statsmodels`) to the world of computer scientists (i.e. machine learning and `scikit-learn`). \n",
    "\n",
    "**(5)** Import the Linear Regression model and instantiate it with code like:\n",
    "\n",
    "```python \n",
    "from sklearn.linear_model import LinearRegression\n",
    "my_model = LinearRegression(fit_intercept=False)\n",
    "```\n",
    "\n",
    "The `fit_intercept=False` tells `scikit-learn` you don't want it to fit an intercept for your model *behind the scenes*. We use `False` because `patsy` included an intercept in our design matrix (a column of 1s), and we want to see the coefficient on that column in our coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now fit your model against X and y and take a look at the coefficients.\n",
    "\n",
    "Machine learning, more than absolutely anything else, is concerned with *predicting* values, and that's evident in what functionality is exposed by this implementation of linear regressions. As you may recall, in `statsmodels`, you could type `.summary()` and get something that looked like this:\n",
    "\n",
    "![statsmodel_output](../images/statsmodel_output.png)\n",
    "\n",
    "A full printout of various diagnostics, all your coefficients, estimates of confidence intervals for each coefficient, etc. \n",
    "\n",
    "By contrast, `LinearRegression` from `sklearn` has no `summary` method. Indeed, the only output you really get for what the model has actually fit is `my_model.coef_`, which will look something like (not real values or number of coefficients you should get here):\n",
    "\n",
    "```python\n",
    "> my_model.fit(X_train, y_train)\n",
    "> my_model.coef_\n",
    "\n",
    "array([[ 7.89347789e+00, -12.3442359e+00,\n",
    "        -8.78152768e+00, 3.215196740e+00]])\n",
    "```\n",
    "\n",
    "Which I think we can all agree is not nearly as informative a print-out! \n",
    "\n",
    "To be clear, you can recover many of the diagnostics for LinearRegression by digging around in other corners of `sklearn`, but what is made available speaks to the prioritizes of different users: `sklearn` is for making predictions; `statsmodels` is for statistics and understanding mechanisms (i.e. seeing if the coefficient on smoking is significant). \n",
    "\n",
    "Store your model's coefficients for smoking in `ex6_model_smoking`. (Note how much trickier this is here than with `statsmodels`! sklearn doesn't think anyone would ever care about individual coefficients).\n",
    "\n",
    "**Note:** In `statsmodels`, the `.fit()` method returned a new fitted model. In `sklearn`, by contrast, `.fit` modifies (mutates) the model in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept and coefficients of the model are 43.509, 4.217, -9.334, -5.715, -0.000, 1.023, 0.122, -9.191.\n"
     ]
    }
   ],
   "source": [
    "model_one.fit(x_train, y_train)\n",
    "coefficients = model_one.coef_\n",
    "results[\"ex6_model_smoking\"] = coefficients[0][7]\n",
    "to_display = \", \".join(map(lambda coe: f\"{coe:.3f}\", coefficients[0]))\n",
    "print(f\"The intercept and coefficients of the model are {to_display}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)** OK, but we're in the world of sklearn, so let's do some prediction! Now that you've fit your model, use the `predict` method your data to create a set of predictions. Store the first (index 0) predicted birthweight in `ex7_first_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first predicted birth weight is 118.77 oz.\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model_one.predict(x_test)\n",
    "first_predicted_bwt = y_predicted[0][0]\n",
    "results[\"ex7_first_prediction\"] = first_predicted_bwt\n",
    "print(f\"The first predicted birth weight is {first_predicted_bwt:.2f} oz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your Model\n",
    "\n",
    "So we now have a trained model that we can use to predict birthweights. Yay! But is it any good? \n",
    "\n",
    "All `sklearn` models have a method called `score` you can used to get the most basic evaluation of your model. The syntax is just:\n",
    "\n",
    "```python\n",
    "my_model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "If you're doing a classification model (something that tries to guess the category for each observation, like a model that evalutes a set of pictures and tries to figure out if the pictures are of cats, dogs, or humans), `score` will return an \"accuracy\" score (the percentage of observations you properly classified). For a regression model (trying to guess a continuous variable) it will give an R-squared score. \n",
    "\n",
    "As you get more sophisticated, you will discover these basic scores are often inadequate for evaluating models, and you can turn to other evaluation functions found in [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter). But for now we'll just use the default `score` output of R-squared. \n",
    "\n",
    "**(8)** What is the score of your model? Store the result as `ex8_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score of the model is 0.074.\n"
     ]
    }
   ],
   "source": [
    "model_score = model_one.score(x_test, y_test)\n",
    "results[\"ex8_score\"] = model_score\n",
    "print(f\"The R-squared score of the model is {model_score:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow Summary\n",
    "\n",
    "Congratulations! You just did you just fit your machine learning algorithm! And you also learned that sometimes what constitutes \"machine learning\" is in the eye of the beholder, given what you did today is the same thing you did in our last class without calling it machine learning. :)\n",
    "\n",
    "But hopefully that's given you a general sense for the work-flow of scikit-learn:\n",
    "\n",
    "1. Prep your data:\n",
    "\n",
    "```python\n",
    "import patsy\n",
    "y , X = patsy.dmatrices('bwt_oz ~ C(race_recoded) + smoke + mage + mpregwt + mht', smoking_and_bw)\n",
    "```\n",
    "\n",
    "2. Split your data:\n",
    "\n",
    "```python \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "```\n",
    "\n",
    "3. Import and fit a model:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "my_model = LinearRegression()\n",
    "my_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "4. Evaluate your model:\n",
    "\n",
    "```python\n",
    "my_model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "5. Use youre model to make predictions:\n",
    "\n",
    "```python\n",
    "my_predictions = my_model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comparing Models\n",
    "\n",
    "Now that we have a baseline estimate for the performance of `LinearRegression` for this set of features and outputs, let's try a different model and see how it compares! \n",
    "\n",
    "**(9)** Now repeat your analysis using a Support Vector Regression (`from sklearn.svm import SVR`). \n",
    "\n",
    "How does the model perform? Is it better or worse than LinearRegression?  \n",
    "\n",
    "\n",
    "**Note:** You'll probably get a `DataConversionWarning: A column-vector y was passed when a 1d array was expected`. This is [related to this common gotcha](https://www.practicaldatascience.org/html/41_broadcasting.html#A-Common-Gotcha:-Narrow-Matrices-v.-1-Dimensional-Vectors). Make sure you understand the problem / are able to resolve the warning. \n",
    "\n",
    "Why didn't you get this with `LinearRegression`? That I can't say! They must each be implemented slightly differently beneath the hood.\n",
    "\n",
    "**ROUND** your value of the score of this fit model result to *2* decimal places and store as `\"ex9_SVR_score\"`. (SVR is not deterministic, and it doesn't seem to respect our random seeds, so values beyond this may differ a little across computers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score of the SVR model is 0.02.\n"
     ]
    }
   ],
   "source": [
    "model_two = SVR()\n",
    "y_train_array = y_train.ravel()\n",
    "model_two.fit(x_train, y_train_array)\n",
    "y_predicted_m2 = model_two.predict(x_test)\n",
    "model_two_score = model_two.score(x_test, y_test)\n",
    "model_two_score = round(model_two_score, 2)\n",
    "results[\"ex9_SVR_score\"] = model_two_score\n",
    "print(f\"The R-squared score of the SVR model is {model_two_score}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10)** One choice parameter for SVRs is the kernel it uses for weighting (again, this isn't a class on machine learning, so don't worry too much about what this means—just know that it's a parameter of the model). Check the SVR documentation to figure out how to set the kernel to `linear` and see how it performs now.\n",
    "\n",
    "Store the score of the SVR with linear kernel as `\"ex10_SVR_linear_score\"`. **ROUND** your value of the score of the SVR with linear kernel to *2* decimal places and store as `\"ex10_SVR_linear_score\"` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(11)** Now pick whatever regression model you'd like and see how it performs ([some suggestions](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)). Play with your model specifications and see how well you can do with your new model of one of the ones we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score of the SVR model with kenel set to linear is 0.08.\n"
     ]
    }
   ],
   "source": [
    "model_three = SVR(kernel=\"linear\")\n",
    "model_three.fit(x_train, y_train_array)\n",
    "y_predicted_m3 = model_three.predict(x_test)\n",
    "model_three_score = model_three.score(x_test, y_test)\n",
    "model_three_score = round(model_three_score, 2)\n",
    "results[\"ex10_SVR_linear_score\"] = model_three_score\n",
    "print(\n",
    "    f\"The R-squared score of the SVR model with kernel set to linear is {model_three_score}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ex10_SVR_linear_score': 0.08,\n",
      " 'ex3_num_columns': 8,\n",
      " 'ex6_model_smoking': -9.190562944229763,\n",
      " 'ex7_first_prediction': 118.77419618919208,\n",
      " 'ex8_score': 0.07427937766715598,\n",
      " 'ex9_SVR_score': 0.02}\n"
     ]
    }
   ],
   "source": [
    "assert set(results.keys()) == {\n",
    "    \"ex3_num_columns\",\n",
    "    \"ex6_model_smoking\",\n",
    "    \"ex7_first_prediction\",\n",
    "    \"ex8_score\",\n",
    "    \"ex9_SVR_score\",\n",
    "    \"ex10_SVR_linear_score\",\n",
    "}\n",
    "pp.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want More Practice?\n",
    "\n",
    "Try replicating our attempts to predict whether infants would be born premature from the [statsmodels exercises](Exercise_statsmodels.ipynb) in scikit-learn. Start with a LogisticRegression, then try some different \"classification models\" for comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75357192 -0.01211707  0.21321507 -1.18240916]]\n",
      "The model's accuracy score is 0.97, which is good.\n",
      "The coefficent for smoking is 0.21. Which is quite different from the 0.425 from the statsmodels exercise. \n",
      "I realized that the coefficents change depending on the size of the training data.\n"
     ]
    }
   ],
   "source": [
    "more_df = df.copy()\n",
    "more_df[\"premature\"] = (df[\"gestation\"] < 252).astype(int)\n",
    "more_df[\"mrace\"] = np.where(df[\"mrace\"] == 1, 1, 0)\n",
    "more_target_vector, more_feature_matrix = pt.dmatrices(\n",
    "    \"premature ~ mpregwt + smoke + mrace\", more_df\n",
    ")\n",
    "more_x_train, more_x_test, more_y_train, more_y_test = train_test_split(\n",
    "    more_feature_matrix,\n",
    "    more_target_vector,\n",
    "    test_size=0.1,\n",
    "    train_size=0.9,\n",
    "    random_state=42,\n",
    ")\n",
    "model_four = LogisticRegression(fit_intercept=False)\n",
    "model_four.fit(more_x_train, more_y_train.ravel())\n",
    "more_y_predicted = model_four.predict(more_x_test)\n",
    "more_model_score = model_four.score(more_x_test, more_y_test)\n",
    "\n",
    "print(model_four.coef_)\n",
    "\n",
    "more_coefficient_smoke = model_four.coef_[0][2]\n",
    "print(f\"The model's accuracy score is {more_model_score:.2f}, which is good.\")\n",
    "print(\n",
    "    f\"The coefficent for smoking is {more_coefficient_smoke:.2f}. Which is quite different from the 0.425 from the statsmodels exercise. \"\n",
    "    + \"\\nI realized that the coefficents change depending on the size of the training data.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
